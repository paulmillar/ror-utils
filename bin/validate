#!/usr/bin/env python3
#
#  Verify ROR JSON data dump.
#

import json
import requests
import concurrent.futures
import sys
from requests_futures.sessions import FuturesSession
from urllib.parse import urljoin
from urllib3.exceptions import (MaxRetryError, SSLError)
import argparse
import validators
import logging

# Controlled vocabularies.  The values are taken from:
#
# https://ror.readme.io/docs/ror-data-structure

VALID_ORG_TYPES = ["Education",
                   "Healthcare",
                   "Company",
                   "Archive",
                   "Nonprofit",
                   "Government",
                   "Facility",
                   "Other" ]

VALID_ORG_STATUSES = [
    "active", # Currently all entries have this value.
    "Active",
    "Inactive",
    "Withdrawn" ]

VALID_ORG_RELATIONSHIP_TYPES = [ "Related",
                                 "Parent",
                                 "Child" ]

# Global list of pending URL-checking futures.
futures_not_done = []


def is_scheme_only_redirect(before_url, after_url):
    """Check whether a redirection involves changing only the scheme.  For
    example, a redirection from "http://example.org/page1.html" to
    "https://example.org/page1.html" is a scheme-only redirection.

    """

    before = before_url.split(':',1)
    after = after_url.split(':',1)
    if len(before) != 2:
        logging.warning("Bad source URL: %s", before_url)
        return False # REVISIT: or raise exception?
    if len(after) != 2:
        logging.warning("Bad redirection URL: %s", after_url)
        return False # REVISIT: or raise exception?
    return after[1] == before [1]


def redirection(response):
    """Use heuristics to establish whether the response to an HTTP HEAD
    request indicates that the URL should be updated. Returns the
    updated URL where appropriate, None otherwise.

    In an ideal world, this method would simply check whether the
    status code is 301 or 308.  However, there have been cases where
    "obvious" permanent redirections (e.g., http:// --> https://) are
    reported as a temporary redirection.  Likewise, there are websites
    that use a permanent redirection to have the client include a
    session key in the URL.

    """
    if not response.is_redirect:
        return None

    #  The 'Expires' header indicates the redirection is time-limited.
    #  The 'Cache-Control' header (depending on the value) may also
    #  result in the redirection being time-limted.
    #
    #  Normally, including an expiry date would be an indication that
    #  an entry's link should NOT be updated.  However, it seems to be
    #  common practice to include an expiry date, even for permanent
    #  redirection and for simple http-->https scheme-only
    #  redirection.  Speculating, this may be to provide a way to
    #  recover from a misconfigured server that redirects to the wrong
    #  URL.
    #
    #  Therefore, we must ignore the the Expires and Cache-Control
    #  headers.

    location_header = response.headers['location']
    location = urljoin(response.url, location_header)

    if response.is_permanent_redirect:
        # Check if the redirection has added something that looks like
        # a session ID (e.g., JSESSIONID).  If so, then ignore the
        # redirection.  If both the original URL and the redirection
        # target (location) have a session then accept the
        # redirection.
        if "session" in location.lower():
            if not "session" in response.url.lower():
                logging.warning("Vetoing redirection %s --> %s: adds a session token", response.url, location)
                return None
            if not is_scheme_only_redirect(response.url, location):
                logging.info("Accepting redirection %s --> %s: existing session token", response.url, location)

        return location
    elif is_scheme_only_redirect(response.url, location):
        logging.debug("Promoting scheme-only redirection %s  -->  %s", response.url, location)
        return location

    return None


def build_missing_value_problem(**extra):
    problem = {}
    problem["problem_class"] = "MISSING VALUE"
    problem["description"] = "A required value is missing"
    for k,v in extra.items():
        problem[k] = v
    return problem


def build_problem(value, problem_class, description, **extra):
    problem = {}
    problem["value"] = value
    if problem_class:
        problem["problem_class"] = problem_class
    problem["description"] = description
    for k,v in extra.items():
        problem[k] = v
    return problem


def append_problem(org_problems, label, value, problem_class, description, **extra):
    problem = build_problem(value, problem_class, description, **extra)
    if not label in org_problems:
        org_problems[label] = []
    org_problems[label].append(problem)


def without_parentheses(text):
    if text[0] == '(' and text[-1] == ')':
        return text[1:-1]
    else:
        return text


def without_quotes(text):
    if text[0] == '"' and text[-1] == '"':
        return text[1:-1]
    else:
        return text


def without_trailing(text, char):
    if text[-1] == char:
        return text[0:-1]
    else:
        return text


def check_result(future):
    if future.cancelled():
        return
    problems = future.problems
    label = future.label
    log_success = future.log_success
    verify = future.verify
    if future.done():
        try:
            r = future.result()
            redirect = redirection(r)

            if redirect:
                append_problem(problems, label, r.url,
                               "REDIRECTION",
                               "Server returned a preferred URL.  Scheduling a check for this new URL.",
                               preferred_url=redirect)
                start_head_req(problems, label, redirect,
                               log_success="Link is good.")
            elif not r.ok:
                logging.debug("%s %d %s", r.url, r.status_code, r.reason)

                if r.status_code == 401:
                    append_problem(problems, label, r.url,
                                   "PERMISSION",
                                   "The server requires authentication to allow access to the resource.")
                if r.status_code == 403:
                    append_problem(problems, label, r.url,
                                   "PERMISSION",
                                   "The server understood the request, but is refusing to authorize it.")
                elif r.status_code == 404:
                    append_problem(problems, label, r.url,
                                   "MISSING",
                                   "The server could not find this resource, which may be because of a temporary problem.")
                elif r.status_code == 410:
                    append_problem(problems, label, r.url,
                                   "MISSING",
                                   "The server asserts this resource has been permanently removed.")
                else:
                    # REVISIT: do we want more specific cases here?
                    append_problem(problems, label, r.url,
                                   "HTTP",
                                   "Web-server indicated a problem: %d %s"
                                   % (r.status_code, r.reason))
            elif log_success:
                append_problem(problems, label, r.url, None,
                               log_success)
        except requests.exceptions.ConnectionError as ce:
            logging.debug("%s %s", ce.request.url, ce)
            cause = ce.__context__
            if type(cause) is MaxRetryError:
                cause = cause.reason
            if type(cause) is SSLError:
                tls_problem = without_quotes(without_trailing(without_parentheses(str(cause)), ","))
                description = "TLS-related problem connecting to web-server: " + tls_problem
                if verify:
                    description = description + ".  Scheduling a new check with certificate verification switched off.  This is ONLY to see if the server redirects to a valid URL."
                    start_head_req(problems, label, ce.request.url,
                                   verify=False, log_success="The server DID NOT redirect to a new URL.  This URL is BAD and SHOULD NOT be used.")
            else:
                connection_problem = without_parentheses(str(cause))
                description = "Problem connecting to web-server: " + connection_problem
            append_problem(problems, label, ce.request.url,
                           "CONNECTION", description)
        except requests.exceptions.Timeout as t:
            append_problem(problems, label, t.request.url,
                           "TIMEOUT",
                           "Web-server took too long to reply: " + str(t))
        except requests.exceptions.RequestException as re:
            logging.debug("%s %s", re.request.url, re)
            append_problem(problems, label, re.request.url,
                           "GENERIC",
                           "Something went wrong when contacting the web-server: " + str(re))
    else:
        logging.warning("Weird future state %s", future)
        append_problem(problems, label, r.url,
                       "INTERNAL",
                       "Weird future: " + str(future))


def start_head_req(problems, label, url, log_success=None,
                   verify=True):
    global futures_not_done
    future = session.head(url, timeout=args.timeout, verify=verify)
    future.label = label
    future.problems = problems
    future.log_success = log_success
    future.verify = verify
    future.add_done_callback(check_result)
    futures_not_done.append(future)


def is_url_valid(url):
    result = validators.url(url, public=True)
    if isinstance(result, validators.ValidationFailure):
        return False
    return result


def validate_url(problems, label, url):
    if not is_url_valid(url):
        logging.debug("Invalid URL \"%s\"", url)
        append_problem(problems, label, url, "BAD VALUE",
                          "The value is not a valid URL.")
        return

    if not args.offline:
        start_head_req(problems, label, url)


def validate_urls(org_problems, org):
    for label, url in urls_to_verify(org):
        validate_url(org_problems, label, url)


def load_json(filename):
    with open(filename) as f:
        logging.info("Loading %s", filename)
        return json.load(f)


def urls_to_verify(org):
    urls = []
    i = 1
    for url in org["links"]:
        label = "links[%d]" % i
        urls.append((label,url))
        i += 1
    wikipedia = org["wikipedia_url"]
    if wikipedia:
        label = "wikipedia_url"
        urls.append((label,wikipedia))
    return urls


def print_progress(total, done):
    logging.info("    %d (%d%%) URL checks completed, of %d total checks.",
                 done, 100.0*done/total, total)


def is_established_year_ok(id, year):
    """Use heuristics to check whether a year is "likely"
    """
    if id == "https://ror.org/021v42516": # if Glastonbury Abbey
        return year == 712
    if id == "https://ror.org/05htk5m33": # if Hunan University
        return year == 976
    if id == "https://ror.org/05fnp1145": # if Al-Azhar University
        return abs(year-972) < 10  # circa 972
    return year >= 1000


def validate_org(org_problems, org, name_by_id):
    if "types" in org:
        i = 1
        for type in org["types"]:
            if not type in VALID_ORG_TYPES:
                label = "types[%d]" % i
                problem = build_problem(type, "BAD VALUE",
                                       "Not one of the allowed values")
                org_problems[label] = problem
            i += 1
    else:
        org_problems["types"] = build_missing_value_problem()


    if "status" in org:
        status = org["status"]
        if not status in VALID_ORG_STATUSES:
            problem = build_problem(status, "BAD VALUE",
                                   "Not one of the allowed values")
            org_problems["status"] = problem
    else:
        org_problems["status"] = build_missing_value_problem()


    if "established" in org:
        established = org["established"]
        if not isinstance(established, int):
            problem = build_problem(established, "BAD VALUE",
                                   "The value is not a number")
            org_problems["established"] = problem
        elif not is_established_year_ok(org["id"], established):
            problem = build_problem(established, "BAD VALUE",
                                   "Year is not (ikely) to be correct")
            org_problems["established"] = problem
    else:
        org_problems["established"] = build_missing_value_problem()


    if "relationships" in org:
        i = 1
        for relationship in org["relationships"]:
            type = relationship["type"]
            if not type in VALID_ORG_RELATIONSHIP_TYPES:
                append_problem(org_problems, "relationships[%d].type" % i,
                               type, "BAD VALUE",
                              "Not one of the allowed values")
            label = relationship["label"]
            id = relationship["id"]
            if not id in name_by_id:
                problem = build_problem(id, "BAD VALUE",
                                        "No corresponding organisation entry")
                org_problems["relationships[%d].id" % i] = problem
            elif name_by_id[id] != label:
                problem = build_problem(label, "BAD VALUE",
                                        "Does not match corresponding organisation name.")
                org_problems["relationships[%d].label" % i] = problem
            i += 1
    else:
        org_problems["relationships"] = build_missing_value_problem()


def update_progress(timeout):
    global futures_not_done
    wait_for = futures_not_done.copy()
    (done,ignored) = concurrent.futures.wait(wait_for, timeout=timeout)
    futures_not_done = [f for f in futures_not_done if f not in done]
    return len(done)


def validate_json(data):
    global futures_not_done
    if not args.offline:
        logging.info("Scheduling URL checks for %d organisations.", len(data))

    name_by_id = {}
    for org in data:
        id = org["id"]
        name = org["name"]
        name_by_id[id] = name

    problems = {}
    try:
        completed_checks_count=0
        for org in data:
            id = org["id"]
            org_problems = {}
            problems[id] = org_problems
            validate_urls(org_problems, org)
            validate_org(org_problems, org, name_by_id)

        logging.info("%d URL checks have been queued.", len(futures_not_done))
        while True:
            done_count = update_progress(2)
            if len(futures_not_done) == 0:
                break
            completed_checks_count += done_count
            total_checks = completed_checks_count + len(futures_not_done)
            print_progress(total_checks, completed_checks_count)
        return problems;

    except KeyboardInterrupt as e:
        logging.warning("Shutting down...")
        done_count = update_progress(0)
        completed_checks_count += done_count
        total_checks = completed_checks_count + len(futures_not_done)
        executor.shutdown(wait=True, cancel_futures=True)
        logging.info("Interrupted when %d%% URL checks have completed.",
                     100.0*(total_checks-len(futures_not_done))/total_checks)
        return problems;


def post_process_problems(problems):
    filtered_problems = {}
    for key, value in problems.items():
        if bool(value):
            filtered_problems[key] = value
    return filtered_problems


def save_problems(problems):
    with open(args.o, 'w') as out:
        json.dump(problems, out, indent=4)


parser = argparse.ArgumentParser(description='Verify a ROR data-dump.')
parser.add_argument('-o', metavar='FILE', default="ror-report.json",
                    help="where to write the output.  If not specified then results are written to 'ror-report.json'.")
parser.add_argument('--offline', required=False,  action='store_true',
                    help="suppress any online tests.")
parser.add_argument('--concurrency', required=False, type=int, metavar='N',
                    default=40,
                    help="The number of HTTP requests to run in parallel.")
parser.add_argument('--timeout', required=False, type=int, metavar='SEC',
                    default=10,
                    help="The maximum time (in seconds) to allow for a request.")
parser.add_argument('--debug', required=False, action='store_true',
                    help="Provide more information about activity.")
parser.add_argument('data', nargs='?', metavar='FILE', help="the ROR data-dump file.  If not specified then 'latest-ror-data.json' is used.",
                    default='latest-ror-data.json')

args = parser.parse_args()

logging.getLogger().setLevel(logging.DEBUG if args.debug else logging.INFO)
logging.getLogger('urllib3').setLevel(logging.INFO)
logging.getLogger('urllib3.connection').setLevel(logging.ERROR)
requests.packages.urllib3.disable_warnings()

concurrency = args.concurrency
logging.info("Running %d HTTP requests in parallel.", concurrency)
executor = concurrent.futures.ThreadPoolExecutor(max_workers=concurrency)
session = FuturesSession(executor=executor)

data = load_json(args.data)
raw_problems = validate_json(data)
problems = post_process_problems(raw_problems)
save_problems(problems)
