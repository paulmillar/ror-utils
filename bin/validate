#!/usr/bin/env python3
#
#  Verify ROR JSON data dump.
#

import json
import requests
import concurrent.futures
import sys
from requests_futures.sessions import FuturesSession
from urllib.parse import urljoin
import argparse
import validators

def redirection(response):
    """Use heuristics to establish whether the response to an HTTP HEAD
    request indicates that the URL should be updated. Returns the
    updated URL where appropriate, None otherwise.

    In an ideal world, this method would simply check whether the
    status code is 301 or 308.  However, there have been cases where
    "obvious" permanent redirections (e.g., http:// --> https://) are
    reported as a temporary redirection.  Likewise, there are websites
    that use a permanent redirection to have the client include a
    session key in the URL.

    """
    if not response.is_redirect:
        return None

    location_header = response.headers['location']
    location = urljoin(response.url, location_header)

    if response.is_permanent_redirect:
        # e.g., JSESSIONID
        if "session" in location.lower():
            # REVISIT, if "session" exists in initial URL then it may
            # be OK.
            print("Vetoing redirection %s  -->  %s  as it contains \"session\"" % (response.url, location))
            return None

        return location
    else:
        before = response.url.split(':',1)
        after = location.split(':',1)
        if len(before) != 2:
            print("Bad source URL: %s" % response.url)
            return None
        if len(after) != 2:
            print("Bad redirection URL: %s" % location)
            return None
        if after[1] == before [1]:
            #print("Promoting scheme-only redirection %s  -->  %s" % (response.url, location))
            return location

    return None


def check_result(future):
    if future.cancelled():
        return
    org = future.ror_org
    id = org["id"]
    if future.done():
        try:
            r = future.result()
            redirect = redirection(r)

            if redirect:
                # FIXME: should schedule another check, verifying the
                # updated URL; for example, does the new location
                # result in 404 status code?
                org["bad_urls"][r.url] = "redirection to %s" % redirect

            elif not r.ok:
                org["bad_urls"][r.url] = "HTTP status code %d %s" % (r.status_code, r.reason)
                #print("%s  %s  %d %s" % (id, r.url, r.status_code, r.reason))
        except requests.exceptions.ConnectionError as ce:
            org["bad_urls"][ce.request.url] = "Error connecting: %s" % ce
            #print("%s  %s %s" % (id, ce.request.url, ce))
        except requests.exceptions.Timeout as t:
            org["bad_urls"][t.request.url] = "Timeout making HTTP request: %s" % t
        except requests.exceptions.RequestException as re:
            org["bad_urls"][re.request.url] = "Generic problem: %s" % re
            #print("%s  %s %s" % (id, re.request.url, re))
    else:
        org["bad_urls"][r.url] = "Weird  future %s" % future
        #print("Weird future state %s" % future)


def start_head_req(org, url):
    future = session.head(url, timeout=args.timeout)
    future.ror_org = org
    future.add_done_callback(check_result)
    return future


def is_url_valid(org, url):
    result = validators.url(url, public=True)
    if isinstance(result, validators.ValidationFailure):
        return False
    return result


def validate_url(org, url):
    if is_url_valid(org, url):
        if not args.offline:
            return start_head_req(org, url)
    else:
        #print("Invalid URL %s  \"%s\"" % (org["id"], url))
        org["bad_urls"][url] = "Invalid URL"
        return None


def validate_urls(org):
    org["bad_urls"]={}
    futures=[]
    for url in urls_to_verify(org):
        maybe_future = validate_url(org, url)
        if maybe_future:
            futures.append(maybe_future)
    return futures


def load_json(filename):
    with open(filename) as f:
        print("Loading %s" % filename)
        return json.load(f)


def urls_to_verify(org):
    urls = []
    for url in org["links"]:
        urls.append(url)
    wikipedia = org["wikipedia_url"]
    if wikipedia:
        urls.append(wikipedia)
    return urls


def print_progress(total, done, data):
    problems=0
    for org in data:
        problems = problems + len(org["bad_urls"])
    print("    %d (%d%%) URL checks completed.  Of the URLs checked, %d (%d%%) have a problem."
          % (done, 100.0*done/total, problems, 100.0*problems/done))

def validate_json(data):
    print("Scheduling URL checks for %d organisations." % len(data))
    try:
        completed_checks_count=0
        not_done = []
        for org in data:
            new_futures = validate_urls(org)
            not_done.extend(new_futures)
        total_checks = len(not_done)
        print("%d URL checks have been queued." % total_checks)
        while True:
            (done, not_done) = concurrent.futures.wait(not_done, timeout=2)
            if len(not_done) == 0:
                break
            completed_checks_count = completed_checks_count + len(done)
            print_progress(total_checks, completed_checks_count, data)

    except KeyboardInterrupt as e:
        (done, not_done) = concurrent.futures.wait(not_done, timeout=0)
        print("Cancelling %d scheduled requests..." % len(not_done))
        for future in not_done:
            future.cancel()
        executor.shutdown()
        print("Clean-up done.")


def show_results(data, output_filename):
    if output_filename:
        original_stdout = sys.stdout
        sys.stdout = open(output_filename, 'w')
    else:
        original_stdout = None

    try:
        bad_url_count = 0
        org_count = 0
        for org in data:
            bad_urls = org["bad_urls"]
            if len(bad_urls) > 0:
                id = org["id"]
                bad_url_count = bad_url_count + len(bad_urls)
                org_count = org_count + 1
                print("\n%s" % id)
                for url, failure in bad_urls.items():
                    print("    %s" % url)
                    print("        %s" % failure)

        print("\n%d bad URLs from %d organisations" % (bad_url_count,org_count))

    finally:
        if original_stdout:
            sys.stdout.close()
            sys.stdout = original_stdout


parser = argparse.ArgumentParser(description='Verify a ROR data-dump.')
parser.add_argument('-o', metavar='FILE', help="where to write the output.  If not specified then results are written to stdout.")
parser.add_argument('--offline', required=False,  action='store_true',
                    help="suppress any online tests.")
parser.add_argument('--concurrency', required=False, type=int, metavar='N',
                    default=40,
                    help="The number of HTTP requests to run in parallel.")
parser.add_argument('--timeout', required=False, type=int, metavar='SEC',
                    default=10,
                    help="The maximum time (in seconds) to allow for a request.")
parser.add_argument('data', nargs='?', metavar='FILE', help="the ROR data-dump file.  If not specified then 'latest-ror-data.json' is used.",
                    default='latest-ror-data.json')

args = parser.parse_args()

concurrency = args.concurrency
print("Running %d HTTP requests in parallel." % concurrency)
executor = concurrent.futures.ThreadPoolExecutor(max_workers=concurrency)
session = FuturesSession(executor=executor)

data = load_json(args.data)

validate_json(data)
show_results(data, args.o)
