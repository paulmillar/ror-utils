#!/usr/bin/env python3
#
#  Verify ROR JSON data dump.
#

import json
import requests
import concurrent.futures
import sys
from requests_futures.sessions import FuturesSession
from urllib.parse import urljoin
import argparse
import validators
import logging

# Global list of pending URL-checking futures.
futures_not_done = []


def is_scheme_only_redirect(before_url, after_url):
    """Check whether a redirection involves changing only the scheme.
    """

    before = before_url.split(':',1)
    after = after_url.split(':',1)
    if len(before) != 2:
        logging.warning("Bad source URL: %s", before_url)
        return False # REVISIT: or raise exception?
    if len(after) != 2:
        logging.warning("Bad redirection URL: %s", after_url)
        return False # REVISIT: or raise exception?
    return after[1] == before [1]


def redirection(response):
    """Use heuristics to establish whether the response to an HTTP HEAD
    request indicates that the URL should be updated. Returns the
    updated URL where appropriate, None otherwise.

    In an ideal world, this method would simply check whether the
    status code is 301 or 308.  However, there have been cases where
    "obvious" permanent redirections (e.g., http:// --> https://) are
    reported as a temporary redirection.  Likewise, there are websites
    that use a permanent redirection to have the client include a
    session key in the URL.

    """
    if not response.is_redirect:
        return None

    location_header = response.headers['location']
    location = urljoin(response.url, location_header)

    if response.is_permanent_redirect:
        # Check if the redirection has added something that looks like
        # a session ID (e.g., JSESSIONID).  If so, then ignore the
        # redirection.  If both the original URL and the redirection
        # target (location) have a session then accept the
        # redirection.
        if "session" in location.lower():
            if not "session" in response.url.lower():
                logging.warning("Vetoing redirection %s --> %s: adds a session token", response.url, location)
                return None
            if not is_scheme_only_redirect(response.url, location):
                logging.info("Accepting redirection %s --> %s: existing session token", response.url, location)

        return location
    elif is_scheme_only_redirect(response.url, location):
        logging.debug("Promoting scheme-only redirection %s  -->  %s", response.url, location)
        return location

    return None


def append_url_result(org_results, label, url, problem, description, **extra):
    result = {}
    result["url"] = url
    result["problem_class"] = problem
    result["description"] = description
    for k,v in extra.items():
        result[k] = v
    if not label in org_results:
        org_results[label] = []
    org_results[label].append(result)


def check_result(future):
    if future.cancelled():
        return
    results = future.results
    label = future.label
    if future.done():
        try:
            r = future.result()
            redirect = redirection(r)

            if redirect:
                append_url_result(results, label, r.url,
                                  "REDIRECTION",
                                  "Server's reply indicates the URL is out-of-date and provided an updated URL",
                                  updated_url=redirect)

                # FIXME: The following start_head_req call doesn't work
                #        for unknown reasons.
                #
                #start_head_req(results, label, redirect)
            elif not r.ok:
                logging.debug("%s %d %s", r.url, r.status_code, r.reason)
                # REVISIT: do we want to be more specific here?  For example, mapping
                # a 404 status code to a distinct problem class?
                append_url_result(results, label, r.url,
                                  "HTTP",
                                  "Web-server's reply indicates a problem: %d %s"
                                  % (r.status_code, r.reason))
        except requests.exceptions.ConnectionError as ce:
            logging.debug("%s %s", ce.request.url, ce)
            append_url_result(results, label, ce.request.url,
                              "CONNECTION",
                              "Problem connecting to web-server: " + str(ce))
        except requests.exceptions.Timeout as t:
            append_url_result(results, label, t.request.url,
                              "TIMEOUT",
                              "Web-server took too long to reply: " + str(t))
        except requests.exceptions.RequestException as re:
            logging.debug("%s %s", re.request.url, re)
            append_url_result(results, label, re.request.url,
                              "GENERIC",
                              "Something went wrong when contacting the web-server: " + str(re))
    else:
        logging.warning("Weird future state %s", future)
        append_url_result(results, label, r.url,
                          "INTERNAL",
                          "Weird future: " + str(future))


def start_head_req(results, label, url):
    global futures_not_done
    future = session.head(url, timeout=args.timeout)
    future.label = label
    future.results = results
    future.add_done_callback(check_result)
    futures_not_done.append(future)


def is_url_valid(url):
    result = validators.url(url, public=True)
    if isinstance(result, validators.ValidationFailure):
        return False
    return result


def validate_url(results, label, url):
    if is_url_valid(url):
        if not args.offline:
            start_head_req(results, label, url)
    else:
        logging.debug("Invalid URL \"%s\"", url)
        append_url_result(results, label, url, "BAD URL",
                          "The text is not a valid URL.")


def validate_urls(org):
    results = {}
    for label, url in urls_to_verify(org):
        validate_url(results, label, url)
    return results


def load_json(filename):
    with open(filename) as f:
        logging.info("Loading %s", filename)
        return json.load(f)


def urls_to_verify(org):
    urls = []
    i = 1
    for url in org["links"]:
        label = "links[%d]" % i
        urls.append((label,url))
        i += 1
    wikipedia = org["wikipedia_url"]
    if wikipedia:
        label = "wikipedia_url"
        urls.append((label,wikipedia))
    return urls


def print_progress(total, done, results):
    problems=0
    for org_results in results.values():
        problems = problems + len(org_results)
    logging.info("    %d (%d%%) URL checks (of %d) completed.  Of the URLs checked, %d (%d%%) have a problem.", done, 100.0*done/total, total, problems, 100.0*problems/done)

def validate_json(data):
    global futures_not_done
    logging.info("Scheduling URL checks for %d organisations.", len(data))
    results = {}
    try:
        completed_checks_count=0
        for org in data:
            org_results = validate_urls(org)
            id = org["id"]
            results[id] = org_results

        logging.info("%d URL checks have been queued.", len(futures_not_done))
        while True:
            (done, futures_not_done) = concurrent.futures.wait(futures_not_done, timeout=2)
            if len(futures_not_done) == 0:
                break
            completed_checks_count += len(done)
            total_checks = completed_checks_count + len(futures_not_done)
            print_progress(total_checks, completed_checks_count, results)
        return results;

    except KeyboardInterrupt as e:
        logging.warning("Shutting down...")
        (done, futures_not_done) = concurrent.futures.wait(futures_not_done, timeout=0)
        executor.shutdown(wait=True, cancel_futures=True)
        completed_checks_count += len(done)
        total_checks = completed_checks_count + len(futures_not_done)
        logging.info("Interrupted when %d%% URL checks have completed.",
                     100.0*(total_checks-len(futures_not_done))/total_checks)
        return results;


def post_process_results(results):
    new_results = {}
    for key, value in results.items():
        if bool(value):
            new_results[key] = value
    return new_results


def show_results(results):
    out = open(args.o, 'w') if args.o else sys.stdout
    try:
        json.dump(results, out, indent=4)
    finally:
        if args.o:
            out.close()


parser = argparse.ArgumentParser(description='Verify a ROR data-dump.')
parser.add_argument('-o', metavar='FILE', help="where to write the output.  If not specified then results are written to stdout.")
parser.add_argument('--offline', required=False,  action='store_true',
                    help="suppress any online tests.")
parser.add_argument('--concurrency', required=False, type=int, metavar='N',
                    default=40,
                    help="The number of HTTP requests to run in parallel.")
parser.add_argument('--timeout', required=False, type=int, metavar='SEC',
                    default=10,
                    help="The maximum time (in seconds) to allow for a request.")
parser.add_argument('--debug', required=False, action='store_true',
                    help="Provide more information about activity.")
parser.add_argument('data', nargs='?', metavar='FILE', help="the ROR data-dump file.  If not specified then 'latest-ror-data.json' is used.",
                    default='latest-ror-data.json')

args = parser.parse_args()

logging.getLogger().setLevel(logging.DEBUG if args.debug else logging.INFO)
logging.getLogger('urllib3').setLevel(logging.INFO)

concurrency = args.concurrency
logging.info("Running %d HTTP requests in parallel.", concurrency)
executor = concurrent.futures.ThreadPoolExecutor(max_workers=concurrency)
session = FuturesSession(executor=executor)

data = load_json(args.data)
results = validate_json(data)
results2 = post_process_results(results)
show_results(results2)
