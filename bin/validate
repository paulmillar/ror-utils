#!/usr/bin/env python3
#
#  Verify ROR JSON data dump.
#

import json
import requests
import concurrent.futures
import sys
from requests_futures.sessions import FuturesSession
from urllib.parse import (urljoin, urlparse)
from urllib3.exceptions import (MaxRetryError, SSLError)
from http import cookiejar
import argparse
import validators
import logging
import time

# Controlled vocabularies.  The values are taken from:
#
# https://ror.readme.io/docs/ror-data-structure

VALID_ORG_TYPES = ["Education",
                   "Healthcare",
                   "Company",
                   "Archive",
                   "Nonprofit",
                   "Government",
                   "Facility",
                   "Other" ]

# Currently all ror entries have the status 'active', although this is
# not a valid value.  The correct value is 'Active'.
VALID_ORG_STATUSES = [
    "active",
    "Active",
    "Inactive",
    "Withdrawn" ]

VALID_ORG_RELATIONSHIP_TYPES = [ "Related",
                                 "Parent",
                                 "Child" ]

# Global list of pending URL-checking futures.
futures_not_done_count = 0
futures_done_count = 0

# Whether the HTTP HEAD request result callback is active.
callback_active = 0

# Global loop detection
loop_urls = {}

# CookieJars for links being tested
cookiejars = {}

# Feedback state
prev_done = 0
prev_total = 0


class BlockAll(cookiejar.CookiePolicy):
    return_ok = set_ok = domain_return_ok = path_return_ok = lambda self, *args, **kwargs: False
    netscape = True
    rfc2965 = hide_cookie2 = False


def get_cookiejar(id, label):
    global cookiejars
    key = id + label
    if not key in cookiejars:
        cookiejars[key] = cookiejar.CookieJar()
    return cookiejars[key]


def clear_cookiejar(id, label):
    global cookiejars
    key = id + label
    if key in cookiejars:
        jar = cookiejars.pop(key)
        jar.clear()


def default_port(scheme):
    return "80" if scheme == "http" else "443"


def canonical_url(url):
    parsed = urlparse(url)
    if not parsed.port:
        new_netloc = parsed.netloc + ":" + default_port(parsed.scheme)
        parsed = parsed._replace(netloc=new_netloc)
    return parsed


def switch_scheme(url, new_scheme):
    new_netloc = url.netloc.split(":")[0] + ":" + default_port(new_scheme)
    return url._replace(scheme = new_scheme, netloc = new_netloc)


def is_scheme_only_redirect(before_url, after_url):
    """Check whether a redirection involves changing only the scheme.  For
    example, a redirection from "http://example.org/page1.html" to
    "https://example.org/page1.html" is a scheme-only redirection.

    """

    before = canonical_url(before_url)
    after = canonical_url(after_url)
    return switch_scheme(before, after.scheme) == after


def redirection(response, problems, label):
    """Use heuristics to establish whether the response to an HTTP HEAD
    request indicates that the URL should be updated. Returns the
    updated URL where appropriate, None otherwise.

    In an ideal world, this method would simply check whether the
    status code is 301 or 308.  However, there have been cases where
    "obvious" permanent redirections (e.g., http:// --> https://) are
    reported as a temporary redirection.  Likewise, there are websites
    that use a permanent redirection to have the client include a
    session key in the URL.

    """
    if not response.is_redirect:
        return None

    #  The 'Expires' header indicates the redirection is time-limited.
    #  The 'Cache-Control' header (depending on the value) may also
    #  result in the redirection being time-limted.
    #
    #  Normally, including an expiry date would be an indication that
    #  an entry's link should NOT be updated.  However, it seems to be
    #  common practice to include an expiry date, even for permanent
    #  redirection and for simple http-->https scheme-only
    #  redirection.  Speculating, this may be to provide a way to
    #  recover from a misconfigured server that redirects to the wrong
    #  URL.
    #
    #  Therefore, we must ignore the the Expires and Cache-Control
    #  headers.

    location_header = response.headers['location']
    location = urljoin(response.url, location_header)

    if response.is_permanent_redirect:
        # Check if the redirection has added something that looks like
        # a session ID (e.g., JSESSIONID).  If so, then ignore the
        # redirection.  If both the original URL and the redirection
        # target (location) have a session then accept the
        # redirection.
        if "session" in location.lower():
            if not "session" in response.url.lower():
                append_problem(problems, label, response.url,
                               "REDIRECTION",
                               "Vetoing server's preferred URL: it adds a session token.",
                               preferred_url=location)
                return None
            if not is_scheme_only_redirect(response.url, location):
                logging.info("Accepting redirection %s --> %s: existing session token", response.url, location)

        return location
    elif is_scheme_only_redirect(response.url, location):
        logging.debug("Promoting scheme-only redirection %s  -->  %s", response.url, location)
        return location
    else:
        append_problem(problems, label, response.url,
                       "REDIRECTION",
                       "Vetoing server's preferred URL: non-permanent redirection is more than scheme change.",
                       preferred_url=location)

    return None


def build_missing_value_problem(**extra):
    problem = {}
    problem["problem_class"] = "MISSING VALUE"
    problem["description"] = "A required value is missing"
    for k,v in extra.items():
        problem[k] = v
    return problem


def build_problem(value, problem_class, description, **extra):
    problem = {}
    problem["value"] = value
    if problem_class:
        problem["problem_class"] = problem_class
    problem["description"] = description
    for k,v in extra.items():
        problem[k] = v
    return problem


def append_problem(org_problems, label, value, problem_class, description, **extra):
    problem = build_problem(value, problem_class, description, **extra)
    if not label in org_problems:
        org_problems[label] = []
    org_problems[label].append(problem)


def without_parentheses(text):
    if text[0] == '(' and text[-1] == ')':
        return text[1:-1]
    else:
        return text


def without_quotes(text):
    if text[0] == '"' and text[-1] == '"':
        return text[1:-1]
    else:
        return text


def without_trailing(text, char):
    if text[-1] == char:
        return text[0:-1]
    else:
        return text


def strip_from_fqdn(url, prefix):
    parsed = urlparse(url)
    netloc = parsed.netloc
    return parsed._replace(netloc=netloc[len(prefix):]).geturl() if netloc.startswith(prefix) else None


def get_root_cause(connection_error):
    cause = connection_error.__context__
    if type(cause) is MaxRetryError:
        cause = cause.reason
    return cause


def describe_connection_error(connection_error):
    cause = get_root_cause(connection_error)
    if type(cause) is SSLError:
        tls_problem = without_quotes(without_trailing(without_parentheses(str(cause)), ","))
        return "TLS-related problem connecting to web-server: " + tls_problem
    else:
        connection_problem = without_parentheses(str(cause))
        return "Problem connecting to web-server: " + connection_problem


def is_certificate_problem(connection_error):
    """Check whether this connection error is because of some problem with
    the certificate; for example, untrusted CA, certificate has
    expired, or the certificate SANs do not match the FQDN in the URL.
    In effect, this function assesses whether disabling certificate
    verification could result in the request succeeding.

    """

    cause = get_root_cause(connection_error)
    if not type(cause) is SSLError:
        return False

    msg = str(cause)
    return " doesn't match " in msg \
        or "CERTIFICATE_VERIFY_FAILED" in msg



def is_certificate_SAN_problem(connection_error):
    """Check whether this connection error is because the certificate's
    SANs do not match the FQDN in the URL.  In effect, this function
    assesses whether modifying the FQDN in the URL could result in the
    request succeeding.

    """

    cause = get_root_cause(connection_error)
    if not type(cause) is SSLError:
        return False

    return " doesn't match " in str(cause)


def check_result(future):
    global callback_active, futures_done_count, futures_not_done_count
    if future.cancelled():
        return
    problems = future.problems
    label = future.label
    log_success = future.log_success
    verify = future.verify
    id = future.id
    try_alternative_fqdn = future.try_alternative_fqdn
    request_submitted = False

    try:
        assert callback_active >= 0
        callback_active += 1

        if not future.done():
            logging.warning("Weird future state %s", future)
            append_problem(problems, label, r.url,
                           "INTERNAL",
                           "Weird future: " + str(future))
            return

        r = future.result()
        jar = get_cookiejar(id, label)
        requests.cookies.extract_cookies_to_jar(jar, r.request, r.raw)

        redirect = redirection(r, problems, label)

        if redirect:
            append_problem(problems, label, r.url,
                           "REDIRECTION",
                           "Server returned a preferred URL.  Scheduling a check for this preferred URL.",
                           preferred_url=redirect)
            request_submitted = start_head_req(id, problems,
                                               label, redirect,
                                               log_success="URL is good.")
        elif not r.ok:
            logging.debug("%s %d %s", r.url, r.status_code, r.reason)
            extra_args={"status_code": str(r.status_code)}
            if r.reason:
                extra_args["reason_phrase"] = r.reason

            if r.status_code == 401:
                append_problem(problems, label, r.url,
                               "PERMISSION",
                               "The server requires authentication to allow access to the resource.",
                               **extra_args)
            if r.status_code == 403:
                append_problem(problems, label, r.url,
                               "PERMISSION",
                               "The server understood the request, but is refusing to authorize it.",
                               **extra_args)
            elif r.status_code == 404:
                append_problem(problems, label, r.url,
                               "MISSING",
                               "The server could not find this resource (might be a temporary problem).",
                               **extra_args)
            elif r.status_code == 406:
                accept_header = r.request.headers["accept"]
                if accept_header:
                    extra_args["accept_header"] = accept_header
                append_problem(problems, label, r.url,
                               "FAILED CONTENT NEGOTIATION",
                               "The requested resource is capable of generating only content not acceptable according to the 'Accept' headers sent in the request",
                               **extra_args)
            elif r.status_code == 410:
                append_problem(problems, label, r.url,
                               "MISSING",
                               "The server asserts this resource has been permanently removed.",
                               **extra_args)
            elif r.status_code == 500:
                append_problem(problems, label, r.url,
                               "BAD SERVER",
                               "A generic error message, given when an unexpected condition was encountered and no more specific message is suitable.",
                               **extra_args)
            elif r.status_code == 503:
                append_problem(problems, label, r.url,
                               "BAD SERVER",
                               "The server cannot handle the request (because it is overloaded or down for maintenance). Generally, this is a temporary state.",
                               **extra_args)
            else:
                append_problem(problems, label, r.url,
                               "HTTP",
                               "Web-server indicated a problem",
                               **extra_args)
        else:
            if not verify and try_alternative_fqdn:
                #  To get here, the URL required switching off
                #  certificate verification, with the server
                #  responding with a 200 status code.
                #
                #  One possibility is that the certificate is
                #  valid but doesn't cover the FQDN used in the
                #  URL and the server hasn't been configured to
                #  redirect to the desired FQDN.
                #
                #  As a "shot in the dark", if the URL has a FQDN
                #  starting "www." then try without this "www."
                #  prefix.  This is done by scheduling a test with
                #  this prefix removed; e.g., if the URL is
                #  "https://www.example.org" then try with
                #  "https://example.org".
                #
                #  REVISIT: a more general strategy would be to
                #  obtain the certificate from the server and
                #  extract all DNS-based SANs (subject alternative
                #  names).  Select those SANs that have the URL's
                #  FQDN is a subdomain.  Select the longest such
                #  SAN and rewrite the URL to match.
                new_url = strip_from_fqdn(r.url, "www.")
                if new_url:
                    message = log_success + " Guessing " + new_url
                    append_problem(problems, label, r.url, None,
                                   message)

                    request_submitted = start_head_req(id, problems,
                                                       label, new_url,
                                                       log_success="Link is good.")
                    return

            if log_success:
                append_problem(problems, label, r.url, None,
                               log_success)
    except requests.exceptions.ConnectionError as ce:
        logging.debug("%s %s", ce.request.url, ce)
        description = describe_connection_error(ce)
        retry_without_verify = verify and is_certificate_problem(ce)

        if retry_without_verify:
            description += ".  Check for redirection by retrying with certificate verification disabled."

        append_problem(problems, label, ce.request.url,
                       "CONNECTION", description)

        if retry_without_verify:
            request_submitted = start_head_req(id, problems,
                                               label,
                                               ce.request.url,
                                               verify=False,
                                               try_alternative_fqdn=is_certificate_SAN_problem(ce),
                                               log_success="No redirect: this URL is BAD.")

    except requests.exceptions.InvalidURL as iu:
        append_problem(problems, label, future.url,
                       "BAD VALUE",
                       str(iu))
    except requests.exceptions.Timeout as t:
        append_problem(problems, label, t.request.url,
                       "TIMEOUT",
                       "Web-server took too long to reply: " + str(t))
    except requests.exceptions.RequestException as re:
        logging.debug("%s %s", re.request.url, re)
        append_problem(problems, label, re.request.url,
                       "GENERIC",
                       "Something went wrong when contacting the web-server: " + str(re))
    finally:
        callback_active -= 1
        futures_done_count += 1
        futures_not_done_count -= 1
        if not request_submitted:
            # We're finished with this (id,label)
            clear_loop_history(id, label)
            clear_cookiejar(id, label)

    assert callback_active >= 0


def cookie_label(cookie):
    """Calculate a canonical string represention for a cookie"""
    cookie_str = str(cookie.version)
    cookie_str += cookie.name
    cookie_str += str(cookie.value)
    cookie_str += str(cookie.port)
    cookie_str += cookie.path
    return cookie_str


def hash_cookiejar(id, label):
    """Calculate a short value (a hash) that represents a cookiejar's
    content"""
    cookie_labels = []
    for cookie in get_cookiejar(id, label):
        label = cookie_label(cookie)
        cookie_labels.append(label)

    cookie_labels.sort()

    jar_str = " ".join(cookie_labels)
    return hex(hash(jar_str))


def is_loop(id, label, url, verify):
    global loop_urls

    if not id in loop_urls:
        loop_urls[id] = {}

    org_loop_info = loop_urls[id]

    if not label in org_loop_info:
        org_loop_info[label] = []

    label_loop_info = org_loop_info[label]

    cookies = hash_cookiejar(id, label)
    can_url = (url.rstrip("/") + "/") if url[-1] == "/" else url
    verify_str = "V" if verify else "NV"
    head_request = verify_str + " " + can_url + " " + cookies

    if head_request in label_loop_info:
        return True

    label_loop_info.append(head_request)
    return False


def clear_loop_history(id, label):
    global loop_urls
    if not id in loop_urls:
        return

    org_loop_info = loop_urls[id]
    if label in org_loop_info:
        del org_loop_info[label]

    if not org_loop_info:
        del loop_urls[id]


def start_head_req(id, problems, label, url, log_success=None,
                   try_alternative_fqdn=False,
                   verify=True):
    global futures_not_done_count
    if is_loop(id, label, url, verify):
        append_problem(problems, label, url, "LOOP",
                       "Vetoing requesting this URL: we have already tested it.")
        return False
    s = session if verify else insecure_session
    jar = get_cookiejar(id, label)
    try:
        future = s.head(url, timeout=args.timeout, cookies=jar)
    except RuntimeError as e:
        append_problem(problems, label, url, "INTERNAL",
                       "Cannot check this URL: " + str(e))
        return False

    future.url = url
    future.label = label
    future.problems = problems
    future.log_success = log_success
    future.verify = verify
    future.id = id
    future.try_alternative_fqdn = try_alternative_fqdn
    future.add_done_callback(check_result)
    futures_not_done_count += 1
    return True


def is_url_valid(url):
    result = validators.url(url, public=True)
    if isinstance(result, validators.ValidationFailure):
        return False
    return result


def validate_url(id, problems, label, url):
    if not is_url_valid(url):
        logging.debug("Invalid URL \"%s\"", url)
        append_problem(problems, label, url, "BAD VALUE",
                          "The value is not a valid URL.")
        return

    if not args.offline:
        start_head_req(id, problems, label, url)


def validate_urls(org_problems, org):
    for label, url in urls_to_verify(org):
        validate_url(org["id"], org_problems, label, url)


def load_json(filename):
    with open(filename) as f:
        logging.info("Loading %s", filename)
        return json.load(f)


def urls_to_verify(org):
    urls = []
    i = 1
    for url in org["links"]:
        label = "links[%d]" % i
        urls.append((label,url))
        i += 1
    wikipedia = org["wikipedia_url"]
    if wikipedia:
        label = "wikipedia_url"
        urls.append((label,wikipedia))
    return urls


def describe_delta(previous, current):
    return (str(current) + " (+" + str(current - previous) + ")") if previous else str(current)

def print_progress():
    global futures_done_count, futures_not_done_count
    global prev_done, prev_total

    done = futures_done_count
    total = done + futures_not_done_count

    percent_done = 100.0*done/total
    done_description = describe_delta(prev_done, done)
    total_description = describe_delta(prev_total, total)
    logging.info("    %s URL checks completed, %d%% of %s total checks.",
                 done_description, percent_done, total_description)
    prev_done = done
    prev_total = total


def is_established_year_ok(id, year):
    """Use heuristics to check whether a year is "likely"
    """
    if id == "https://ror.org/021v42516": # if Glastonbury Abbey
        return year == 712
    if id == "https://ror.org/05htk5m33": # if Hunan University
        return year == 976
    if id == "https://ror.org/05fnp1145": # if Al-Azhar University
        return abs(year-972) < 10  # circa 972
    return year >= 1000


def validate_org(org_problems, org, name_by_id):
    if "types" in org:
        i = 1
        for type in org["types"]:
            if not type in VALID_ORG_TYPES:
                label = "types[%d]" % i
                problem = build_problem(type, "BAD VALUE",
                                       "Not one of the allowed values")
                org_problems[label] = problem
            i += 1
    else:
        org_problems["types"] = build_missing_value_problem()


    if "status" in org:
        status = org["status"]
        if not status in VALID_ORG_STATUSES:
            problem = build_problem(status, "BAD VALUE",
                                   "Not one of the allowed values")
            org_problems["status"] = problem
    else:
        org_problems["status"] = build_missing_value_problem()


    if "established" in org:
        established = org["established"]
        if not isinstance(established, int):
            problem = build_problem(established, "BAD VALUE",
                                   "The value is not a number")
            org_problems["established"] = problem
        elif not is_established_year_ok(org["id"], established):
            problem = build_problem(established, "BAD VALUE",
                                   "Year is not (ikely) to be correct")
            org_problems["established"] = problem
    else:
        org_problems["established"] = build_missing_value_problem()


    if "relationships" in org:
        i = 1
        for relationship in org["relationships"]:
            type = relationship["type"]
            if not type in VALID_ORG_RELATIONSHIP_TYPES:
                append_problem(org_problems, "relationships[%d].type" % i,
                               type, "BAD VALUE",
                              "Not one of the allowed values")
            label = relationship["label"]
            id = relationship["id"]
            if not id in name_by_id:
                problem = build_problem(id, "BAD VALUE",
                                        "No corresponding organisation entry")
                org_problems["relationships[%d].id" % i] = problem
            elif name_by_id[id] != label:
                problem = build_problem(label, "BAD VALUE",
                                        "Does not match corresponding organisation name.")
                org_problems["relationships[%d].label" % i] = problem
            i += 1
    else:
        org_problems["relationships"] = build_missing_value_problem()


def validate_json(data):
    global futures_done_count, futures_not_done_count
    if not args.offline:
        logging.info("Scheduling URL checks for %d organisations.", len(data))

    name_by_id = {}
    for org in data:
        id = org["id"]
        name = org["name"]
        name_by_id[id] = name

    problems = {}
    try:
        for org in data:
            id = org["id"]
            org_problems = {}
            problems[id] = org_problems
            validate_urls(org_problems, org)
            validate_org(org_problems, org, name_by_id)

        logging.info("%d URL checks have been queued.", futures_not_done_count)
        while True:
            if futures_not_done_count == 0 and callback_active == 0:
                break
            print_progress()
            time.sleep(2)
        return problems;

    except KeyboardInterrupt as e:
        logging.warning("Shutting down...")
        executor.shutdown(wait=True, cancel_futures=True)
        total = futures_done_count + futures_not_done_count
        logging.info("Interrupted when %d%% URL checks have completed.",
                     100.0*(total-futures_not_done_count)/total)
        return problems;


def post_process_problems(problems):
    filtered_problems = {}
    for key, value in problems.items():
        if bool(value):
            filtered_problems[key] = value
    return filtered_problems


def save_problems(problems):
    with open(args.o, 'w') as out:
        json.dump(problems, out, indent=4)


parser = argparse.ArgumentParser(description='Verify a ROR data-dump.')
parser.add_argument('-o', metavar='FILE', default="ror-report.json",
                    help="where to write the output.  If not specified then results are written to 'ror-report.json'.")
parser.add_argument('--offline', required=False,  action='store_true',
                    help="suppress any online tests.")
parser.add_argument('--concurrency', required=False, type=int, metavar='N',
                    default=40,
                    help="The number of HTTP requests to run in parallel.")
parser.add_argument('--timeout', required=False, type=int, metavar='SEC',
                    default=10,
                    help="The maximum time (in seconds) to allow for a request.")
parser.add_argument('--debug', required=False, action='store_true',
                    help="Provide more information about activity.")
parser.add_argument('data', nargs='?', metavar='FILE', help="the ROR data-dump file.  If not specified then 'latest-ror-data.json' is used.",
                    default='latest-ror-data.json')

args = parser.parse_args()

logging.getLogger().setLevel(logging.DEBUG if args.debug else logging.INFO)
logging.getLogger('urllib3').setLevel(logging.INFO)
logging.getLogger('urllib3.connection').setLevel(logging.ERROR)
logging.getLogger('urllib3.connectionpool').setLevel(logging.ERROR)
requests.packages.urllib3.disable_warnings()

concurrency = args.concurrency
logging.info("Running %d HTTP requests in parallel.", concurrency)
executor = concurrent.futures.ThreadPoolExecutor(max_workers=concurrency)
session = FuturesSession(executor=executor)
session.cookies.set_policy(BlockAll())
insecure_session = FuturesSession(executor=executor)
insecure_session.verify = False
insecure_session.cookies.set_policy(BlockAll())

data = load_json(args.data)
raw_problems = validate_json(data)
problems = post_process_problems(raw_problems)
save_problems(problems)
